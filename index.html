<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IG-AE</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>

<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Bringing NeRFs to the Latent Space: <br> Inverse Graphics Autoencoder</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.fr/citations?user=CcjdVBgAAAAJ&hl=fr" target="_blank" rel="noopener noreferrer">Antoine Schnepf</a><sup>* 1,3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=0dObvuYAAAAJ&hl=fr" target="_blank" rel="noopener noreferrer">Karim Kassab</a><sup>* 1,2</sup>,</span>
            <span class="author-block">
              <a href="https://jyfranceschi.fr/" target="_blank" rel="noopener noreferrer">Jean-Yves Franceschi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=N0YTGr8AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Laurent Caraffa</a><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.fr/citations?user=L7qb6ToAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Flavian Vasile</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.fr/citations?user=T3dQRjAAAAAJ&hl=fr" target="_blank" rel="noopener noreferrer">Jeremie Mary</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8UW2vacAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Andrew Comport</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.fr/citations?user=mqq6zX4AAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Valérie Gouet-Brunet</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>* </sup>equal contribution</span><br>
            <span class="author-block"><sup>1 </sup>Criteo AI Lab, Paris, France</span><br>
            <span class="author-block"><sup>2 </sup>LASTIG, Université Gustave Eiffel, IGN-ENSG, F-94160 Saint-Mandé</span><br>
            <span class="author-block"><sup>3 </sup>Université Côte d’Azur, CNRS, I3S, France</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.22936"
                   class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/k-kassab/igae"
                   class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/AntoineSchnepf/latent-nerfstudio"
                   class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener noreferrer">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Latent Nerfstudio</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (coming soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ### -->
<!-- <div class="container is-max-desktop">
  <div class="column is-full-width">
    <div class="columns is-centered">
      <video id="large-scale" autoplay muted loop playsinline width="65%">
        <source src="./static/videos/front_cover.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</div>
</section> -->

<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified">
        <div class="columns is-centered has-text-centered">
          <video id="large-scale" autoplay muted loop playsinline width="65%">
            <source src="./static/videos/front_cover.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          While pre-trained image autoencoders are increasingly utilized in computer vision, the application of inverse graphics in 2D latent spaces has been under-explored. 
          Yet, besides reducing the training and rendering complexity, applying inverse graphics in the latent space enables a valuable interoperability with other latent-based 2D methods.
          The major challenge is that inverse graphics cannot be directly applied to such image latent spaces because they lack an underlying 3D geometry. 
          In this paper, we propose an Inverse Graphics Autoencoder (IG-AE) that specifically addresses this issue.
          To this end, we regularize an image autoencoder with 3D-geometry by aligning its latent space with jointly trained latent 3D scenes. 
          We utilize the trained IG-AE to bring NeRFs to the latent space with a latent NeRF training pipeline, which we implement in an open-source extension of the Nerfstudio framework, thereby unlocking latent scene learning for its supported methods. 
          We experimentally confirm that Latent NeRFs trained with IG-AE present an improved quality compared to a standard autoencoder, all while exhibiting training and rendering accelerations with respect to NeRFs trained in the image space.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- Method. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        We propose a Latent NeRF Training Pipeline compatible with most NeRF architectures and auto-encoders (AEs).
        Subsequently, we identify classical AEs as limiting factors in latent NeRF quality, and propose IG-AE to improve it.<br><br>
        <!-- Latent NeRF Training Pipeline. -->
        <h3 class="title is-4">Latent NeRF Training Pipeline</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <img src="static/images/latent-nerf-training.svg" alt="Latent NeRF Training" width="75%"/>
          </div>
          <p>
            <b>Latent NeRF Training.</b> 
            We train a Latent NeRF in two stages. 
            First, we train the chosen NeRF method \(F_\theta\) using its proprietary loss \(\mathcal{L}_{F_\theta}\) that matches rendered latents \(\tilde{z}_p\) and encoded latents \(z_p\). 
            Subsequently, we align with the scene in the RGB space by adding decoder fine-tuning via \(\mathcal{L}_\mathrm{align}\) that matches ground truth images \(x_p\) and decoded renderings \(\tilde{x}_p\).
          </p>
        </div>
        <br/>
        <!--/ Latent NeRF Training Pipeline. -->

        <!-- IG-AE Training -->
        <h3 class="title is-4">IG-AE Training</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <img src="static/images/ig-ae-training.svg" alt="IG-AE Training" width="75%"/>
          </div>
          <p>
            <b>IG-AE Training.</b>
            We train IG-AE by applying a 3D regularization on its 2D latent space.
            Specifically, we train the encoder \(E_\phi\) and decoder \(D_\psi\) to preserve 3D consistency by supervising them with 3D-consistent latent images.
            We obtain such 3D consistent latent images \(\tilde{z}_{s,p}\) from a set of Triplanes \(\{T_1, ..., T_N\}\) that are simultaneously learnt.
            Each Tri-Plane \(T_i\) learns the latent scene corresponding to the ground truth RGB scene \(M_i\).
            This optimization is done via two reconstructive objectives. 
            In the latent space,  \(\mathcal{L}_\mathrm{latent}\) aligns the Tri-Plane renderings \(\tilde{z}_{s,p}\) with the encoded ground truth view \(z_{s,p}\), updating both the latent Tri-Planes and the encoder.
            In the RGB space, \(\mathcal{L}_\mathrm{RGB}\) aligns the ground truth view \(x_{s,p}\) with decoded rendering \(\tilde{x}_{s,p}\), updating both the latent Tri-Planes and the decoder. 
            In addition, we preserve the auto-encoding performance of our IG-AE by adopting a reconstructive loss on synthetic and real data, via \(\mathcal{L}_\mathrm{ae}^\mathrm{(synth)}\) and \(\mathcal{L}_\mathrm{ae}^\mathrm{(real)}\) respectively.
          </p>
        </div>
        <!--/ IG-AE Training -->
      </div>
    </div>
  </div>
</section>
<!--/ Method -->

<!-- Latent NeRFs. -->
<section class="section">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
          <div class="column is-full-width">
              <h2 class="title is-3">Latent NeRFs</h2>
              Visualization of Latent NeRFs trained on Objaverse using the Vanilla-NeRF architecture, our Latent NeRF Training Pipeline, and IG-AE.
              <br><br>
              <div class="content has-text-justified">
                <div class="columns is-centered has-text-centered">
                    <video id="large-scale" autoplay muted loop playsinline width="90%">
                        <source src="./static/videos/supplementary_videos/vanilla/vanilla_all_scenes_1.mp4"
                                type="video/mp4">
                    </video>   
                </div>
            </div>
            <div class="content has-text-justified">
              <div class="columns is-centered has-text-centered">
                  <video id="large-scale" autoplay muted loop playsinline width="90%">
                      <source src="./static/videos/supplementary_videos/vanilla/vanilla_all_scenes_2.mp4"
                              type="video/mp4">
                  </video>   
              </div>
          </div>
          <div class="content has-text-justified">
            <div class="columns is-centered has-text-centered">
                <video id="large-scale" autoplay muted loop playsinline width="90%">
                    <source src="./static/videos/supplementary_videos/vanilla/vanilla_all_scenes_3.mp4"
                            type="video/mp4">
                </video>   
            </div>
        </div>
          </div>
      </div>
  </div>
</section>
<!--/ Latent NeRFs. -->

<!-- Results. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison with NeRFs trained in a standard latent space</h2>
        This section compares NeRFs trained in a standard latent space of an AE, and in our IG-AE 3D-aware latent space, using our latent NeRF training pipeline implemented as an extension to Nerfstudio.<br><br>
        <!-- Shapenet Bag -->
        <h3 class="title is-4">Shapenet Bag</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="70%">
              <source src="./static/videos/Bags.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Shapenet Bag -->

        <!-- Shapenet Hat -->
        <h3 class="title is-4">Shapenet Hat</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="70%">
              <source src="./static/videos/Hats.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Shapenet Hat -->
        <!-- Shapenet Hat zoomed -->
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/hats_zoomed.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Shapenet Hat zoomed -->

        <!-- Shapenet Vase -->
        <h3 class="title is-4">Shapenet Vase</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="70%">
              <source src="./static/videos/Vases.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Shapenet Vase -->
        <!-- Shapenet vase zoomed -->
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/vases_zoomed.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Shapenet vase zoomed -->
      </div>
    </div>
  </div>
</section>
<!--/ Results -->

<!-- Supplementary Results. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Supplementary results on Objaverse</h2>
        This section illustrates our supplementary results on held-out Objaverse objects.<br><br><br>

        <!-- Objaverse -->
        <h3 class="title is-4">Objaverse Book</h3>
        <div class="content has-text-justified">
            <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
                <source src="./static/videos/supplementary_videos/tensorf/Book.mp4"
                        type="video/mp4">
            </video>   
            </div>
        </div>
        <br/>
        <!--/ Objaverse -->

        <!-- Objaverse -->
        <h3 class="title is-4">Objaverse Burger</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/tensorf/Burger.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Objaverse -->

        <!-- Objaverse -->
        <h3 class="title is-4">Objaverse Flower</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/tensorf/Flower.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Objaverse -->

        <!-- Objaverse -->
        <h3 class="title is-4">Objaverse Painting</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/tensorf/Painting.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Objaverse -->

        <!-- Objaverse -->
        <h3 class="title is-4">Objaverse Robot </h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/tensorf/Robot.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Objaverse -->

        <!-- Objaverse -->
        <h3 class="title is-4">Objaverse Table</h3>
        <div class="content has-text-justified">
          <div class="columns is-centered has-text-centered">
            <video id="large-scale" autoplay muted loop playsinline width="85%">
              <source src="./static/videos/supplementary_videos/tensorf/Table.mp4"
                      type="video/mp4">
            </video>   
          </div>
        </div>
        <br/>
        <!--/ Objaverse -->
      </div>
    </div>
  </div>
</section>
<!--/ Supplementary Results -->

<!-- BibTex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{
        ig-ae,
        title={Bringing Ne{RF}s to the Latent Space: Inverse Graphics Autoencoder},
        author={Antoine Schnepf and Karim Kassab and Jean-Yves Franceschi and Laurent Caraffa and Flavian Vasile and Jeremie Mary and Andrew I. Comport and Valerie Gouet-Brunet},
        booktitle={The Thirteenth International Conference on Learning Representations},
        year={2025},
        url={https://openreview.net/forum?id=LTDtjrv02Y}
      }
    </code></pre>
  </div>
</section>
<!--/ BibTex -->

<!-- Thanks -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was built on top of the <a href="https://github.com/nerfies/nerfies.github.io">following template</a>, for which we thank the authors.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
<!--/ Thanks -->

</body>
</html>
